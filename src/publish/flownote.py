"""
Flow Note generator for Attention Flow Desk.
Generates Daily Close Notes in Markdown format.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Optional
from zoneinfo import ZoneInfo

from jinja2 import Template

from .. import db
from ..config import get_config

# Minimal flow note template
FLOW_NOTE_TEMPLATE = """# ATTENTION FLOW DESK — {{ note_date }}

Generated: {{ generated_at }}

---

## Since Last Close

{% if new_movers %}
{{ new_movers | length }} new top mover(s) since last note.
{% else %}
No new movers since last note.
{% endif %}

---

## Top Movers (Last 24h)

{% if movers %}
| Post | Source | Actor | Age | Velocity (6h) | Flow Score | Snapshots |
|------|--------|-------|-----|---------------|------------|-----------|
{% for m in movers %}
| [{{ m.title | truncate(40) }}]({{ m.url }}) | {{ m.source }} | {{ m.actor_label }} | {{ m.age_display }} | {{ m.velocity_display }} | {{ m.flow_score_display }} | {{ m.snapshot_count }} |
{% endfor %}
{% else %}
*No posts with sufficient data for ranking.*

{% if bootstrap_warning %}
> **Note:** This system needs 2+ ingestion runs before velocity can be calculated.
> Run `python -m src.run ingest` again in 4-6 hours.
{% endif %}
{% endif %}

---

{% if clusters %}
## Emerging Clusters

{% for c in clusters %}
### {{ c.summary or 'Cluster ' + c.cluster_id|string }}

- **Source:** {{ c.source or 'Mixed' }}
- **Posts:** {{ c.member_count }}
- **Unique Actors:** {{ c.unique_actor_count }}
- **Strength:** {{ c.strength | round(2) if c.strength else 'N/A' }}

{% endfor %}
{% else %}
## Emerging Clusters

*No clusters detected in the last 48 hours.*

{% endif %}

---

## Flow Summary

{% if summary_points %}
{% for point in summary_points %}
- {{ point }}
{% endfor %}
{% else %}
- Insufficient data for flow summary. Continue running ingestion to build history.
{% endif %}

---

## Next Watch

{% if watch_points %}
{% for point in watch_points %}
- {{ point }}
{% endfor %}
{% else %}
- Monitor for first velocity calculations after multiple ingestion runs.
{% endif %}

---

## Data Status

- **Last Ingest:** {{ last_ingest or 'Never' }}
- **Snapshots (24h):** {{ snapshots_24h }}
- **Total Posts:** {{ total_posts }}
- **Total Snapshots:** {{ total_snapshots }}

{% if degradation_warnings %}
### Warnings

{% for warning in degradation_warnings %}
- {{ warning }}
{% endfor %}
{% endif %}

---

*Generated by Attention Flow Desk*
"""


def format_age(hours: Optional[float]) -> str:
    """Format post age in human-readable form."""
    if hours is None:
        return "?"
    if hours < 1:
        return f"{int(hours * 60)}m"
    if hours < 24:
        return f"{hours:.1f}h"
    days = hours / 24
    return f"{days:.1f}d"


def format_velocity(velocity: Optional[float]) -> str:
    """Format velocity for display."""
    if velocity is None:
        return "—"
    if velocity >= 1000:
        return f"{velocity/1000:.1f}k/h"
    return f"{velocity:.1f}/h"


def format_score(score: Optional[float]) -> str:
    """Format flow score for display."""
    if score is None:
        return "—"
    return f"{score:.2f}"


def get_movers_data(hours: int = 24, limit: int = 20) -> list[dict]:
    """Get top movers data formatted for the template."""
    raw_movers = db.get_top_movers(hours=hours, limit=limit)

    movers = []
    seen_actors = {}  # Track count per actor (max 2 per actor)

    for m in raw_movers:
        actor = m.get("actor_label", "Unknown")

        # Max 2 per actor
        if seen_actors.get(actor, 0) >= 2:
            continue
        seen_actors[actor] = seen_actors.get(actor, 0) + 1

        movers.append({
            "post_id": m["post_id"],
            "title": m.get("title", "Untitled"),
            "url": m.get("url", "#"),
            "source": m.get("source", "?"),
            "actor_label": actor,
            "age_display": format_age(m.get("post_age_hours")),
            "velocity_display": format_velocity(m.get("velocity_6h")),
            "flow_score_display": format_score(m.get("flow_score")),
            "snapshot_count": m.get("snapshot_count", 0),
            "flow_score": m.get("flow_score"),
        })

    return movers


def get_new_movers(movers: list[dict]) -> list[dict]:
    """Filter movers to only those not in the previous note."""
    last_note = db.get_last_note()
    if not last_note:
        return movers

    previous_posts = set(last_note.get("posts_included", []))
    return [m for m in movers if m["post_id"] not in previous_posts]


def generate_summary_points(movers: list[dict], stats: dict) -> list[str]:
    """Generate flow summary bullet points."""
    points = []

    if not movers:
        return points

    # Count sources
    source_counts = {}
    for m in movers:
        src = m.get("source", "unknown")
        source_counts[src] = source_counts.get(src, 0) + 1

    for src, count in source_counts.items():
        points.append(f"{count} top mover(s) from {src}")

    # Note snapshot density
    if stats.get("snapshots_24h", 0) > 0:
        points.append(
            f"{stats['snapshots_24h']} snapshots captured in the last 24 hours "
            f"across {stats.get('posts_count', 0)} posts"
        )

    return points


def generate_watch_points(movers: list[dict]) -> list[str]:
    """Generate next watch bullet points."""
    if not movers:
        return ["Continue ingestion to build baseline data"]

    points = []

    # Mention top actors
    actors = list(set(m.get("actor_label", "Unknown") for m in movers[:5]))
    if actors:
        points.append(f"Monitor velocity changes for: {', '.join(actors[:3])}")

    points.append("Watch for clustering patterns as data accumulates")

    return points


def check_degradation_warnings(stats: dict) -> list[str]:
    """Check for data quality issues."""
    warnings = []

    if stats.get("snapshots_24h", 0) == 0:
        warnings.append("No snapshots in the last 24 hours - ingestion may be failing")

    if stats.get("runs_count", 0) < 2:
        warnings.append("Fewer than 2 runs recorded - velocity calculation requires multiple runs")

    return warnings


def generate_flow_note(output_dir: Optional[Path] = None) -> dict:
    """
    Generate a Daily Close Note.

    Args:
        output_dir: Directory to write the note. Defaults to config notes_dir.

    Returns:
        Dict with generation status and output path.
    """
    config = get_config()

    if output_dir is None:
        output_dir = config.notes_dir

    output_dir.mkdir(parents=True, exist_ok=True)

    # Get local date for filename
    local_tz = ZoneInfo(config.timezone)
    now_local = datetime.now(local_tz)
    note_date = now_local.strftime("%Y-%m-%d")
    generated_at = now_local.strftime("%Y-%m-%d %H:%M %Z")

    # Gather data
    stats = db.get_db_stats()
    movers = get_movers_data(hours=24, limit=20)
    new_movers = get_new_movers(movers)
    clusters = db.get_recent_clusters(hours=48)

    # Parse clusters members
    for c in clusters:
        if c.get("members_json"):
            c["members"] = json.loads(c["members_json"])

    # Generate content sections
    summary_points = generate_summary_points(movers, stats)
    watch_points = generate_watch_points(movers)
    degradation_warnings = check_degradation_warnings(stats)

    # Bootstrap warning if no velocities computed yet
    bootstrap_warning = stats.get("runs_count", 0) < 2

    # Render template
    template = Template(FLOW_NOTE_TEMPLATE)
    content = template.render(
        note_date=note_date,
        generated_at=generated_at,
        movers=movers,
        new_movers=new_movers,
        clusters=clusters,
        summary_points=summary_points,
        watch_points=watch_points,
        degradation_warnings=degradation_warnings,
        bootstrap_warning=bootstrap_warning,
        last_ingest=stats.get("last_run_at"),
        snapshots_24h=stats.get("snapshots_24h", 0),
        total_posts=stats.get("posts_count", 0),
        total_snapshots=stats.get("snapshots_count", 0),
    )

    # Write file
    output_path = output_dir / f"{note_date}.md"
    with open(output_path, "w") as f:
        f.write(content)

    # Record in note history
    posts_included = [m["post_id"] for m in movers]
    clusters_included = [c["cluster_id"] for c in clusters]
    db.record_note(
        note_date=note_date,
        output_path=str(output_path),
        posts_included=posts_included,
        clusters_included=clusters_included
    )

    return {
        "success": True,
        "output_path": str(output_path),
        "note_date": note_date,
        "movers_count": len(movers),
        "clusters_count": len(clusters),
    }
